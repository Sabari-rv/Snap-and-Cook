{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1822d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from requests.exceptions import ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067075aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ce10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_valid(url):\n",
    "   #Checks if the url is valid or not\n",
    "    parsedURL = urlparse(url)\n",
    "    return bool(parsedURL.netloc) and bool(parsedURL.scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa61948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function gives all urls\n",
    "count = 0\n",
    "def get_all_urls(url):\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol (umd.edu in this case)\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            #href is empty and we don't need that a element\n",
    "            continue\n",
    "        #if the link is not absolute, make it by joining relative to the base\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        #constructing an absolute URL from parsed data\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_url_valid(href):\n",
    "            #in valid url\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            #it is already in the set, so we don't need to add\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            #it is an external link. i.e\n",
    "            # Check if it is already there \n",
    "            if href not in external_urls:\n",
    "#                 print(f\"[EXT] External link: {href}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "#         print(\".\",end=\" \")\n",
    "        global count\n",
    "        count = count+1\n",
    "        print(f\"{count}[INT] Internal link: {href}\")\n",
    "\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fec1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_urls_visited = 0\n",
    "def crawl(url, max_urls=30000):\n",
    "    #Max URL is just to decrease the time if there are a lot of pages.\n",
    "    #The following code was openly available of github and I found this\n",
    "    #idea useful to inhibit crawling time\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    try:\n",
    "        links = get_all_urls(url)\n",
    "        for link in links:\n",
    "            if total_urls_visited > max_urls:\n",
    "                break\n",
    "            crawl(link, max_urls=max_urls)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d207e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"https://umd.edu/virusinfo\"\n",
    "#Input\n",
    "# base_url = input(\"Enter the URL : \")\n",
    "base_url = \"https://www.archanaskitchen.com/\"\n",
    "internal_urls.add(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c0c332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='www.archanaskitchen.com', path='/', params='', query='', fragment='')\n",
      "https://www.archanaskitchen.com\n",
      "www.archanaskitchen.com/\n",
      "www.archanaskitchen.com\n"
     ]
    }
   ],
   "source": [
    "parsedurl = urlparse(base_url)\n",
    "print(parsedurl)\n",
    "base_url_main = parsedurl.scheme+\"://\"+parsedurl.netloc\n",
    "print(base_url_main)\n",
    "# base_url_text = base_url.split(\"//\",1)[1]\n",
    "base_url_text = parsedurl.netloc+parsedurl.path\n",
    "print(base_url_text)\n",
    "# base_url_text_domain = base_url_text.split(\"/\",1)[0]\n",
    "base_url_text_domain = parsedurl.netloc\n",
    "print(base_url_text_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6658d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'external_urls' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'external_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m crawl(base_url)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Total External links:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(external_urls))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Total Internal links:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(internal_urls))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Total:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(external_urls) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(internal_urls))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'external_urls' is not defined"
     ]
    }
   ],
   "source": [
    "crawl(base_url)\n",
    "print(\"[+] Total External links:\", len(external_urls))\n",
    "print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "print(\"[+] Total:\", len(external_urls) + len(internal_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf98855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting List to Column\n",
    "df = DataFrame(internal_urls,columns=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626e0657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.archanaskitchen.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                URL\n",
       "0  https://www.archanaskitchen.com/"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b4bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrecipe = df[df['URL'].str.contains('recipe')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed80fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrecipe.to_csv('filename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcd31038",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Find the HTML elements that contain the data you want to scrape\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#recipe = soup.find(\"div\", {\"class\": \"col-md-4 recipe\"}).find(\"a\")\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m recipe_url \u001b[38;5;241m=\u001b[39m recipe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m recipe_title \u001b[38;5;241m=\u001b[39m recipe\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost-title\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     18\u001b[0m recipe_author \u001b[38;5;241m=\u001b[39m recipe\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost-author\u001b[39m\u001b[38;5;124m\"\u001b[39m})\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recipe' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP request to the website's URL\n",
    "url = \"https://www.archanaskitchen.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Get the HTML content of the website\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find the HTML elements that contain the data you want to scrape\n",
    "#recipe = soup.find(\"div\", {\"class\": \"col-md-4 recipe\"}).find(\"a\")\n",
    "recipe_url = recipe[\"href\"]\n",
    "recipe_title = recipe.find(\"h2\", {\"class\": \"post-title\"}).text\n",
    "recipe_author = recipe.find(\"div\", {\"class\": \"post-author\"}).find(\"span\").text\n",
    "recipe_description = recipe.find(\"div\", {\"class\": \"post-content\"}).text.strip()\n",
    "\n",
    "# Print the scraped data\n",
    "print(\"Recipe URL:\", recipe_url)\n",
    "print(\"Recipe Title:\", recipe_title)\n",
    "print(\"Recipe Author:\", recipe_author)\n",
    "print(\"Recipe Description:\", recipe_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18dd7a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send an HTTP request to the website's URL\n",
    "url = \"https://www.archanaskitchen.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Get the HTML content of the website\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all the HTML elements that contain the recipe data\n",
    "recipes = soup.find_all(\"div\", {\"class\": \"col-md-4 recipe\"})\n",
    "\n",
    "# Loop through each recipe element and extract its data\n",
    "for recipe in recipes:\n",
    "    recipe_url = recipe.find(\"a\")[\"href\"]\n",
    "    recipe_title = recipe.find(\"a\").find(\"h2\", {\"class\": \"post-title\"}).text.strip()\n",
    "    print(\"Recipe Title:\", recipe_title)\n",
    "    print(\"Recipe URL:\", recipe_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d8509b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e584e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
